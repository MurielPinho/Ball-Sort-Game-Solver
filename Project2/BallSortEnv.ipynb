{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, error, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "%run 'solver.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TUBES = 5\n",
    "N_COLORS = 4\n",
    "H_TUBES = 4\n",
    "\n",
    "DEFAULT_BOARD = [[3,2,1,3],[2,1,1,2],[1,2,3,4],[4,4,0,0],[3,4,0,0]]\n",
    "arrCompleted = [0]*N_TUBES\n",
    "Board = copy.deepcopy(DEFAULT_BOARD)\n",
    "for arr in Board:\n",
    "    for a in range(len(arr)-1,-1,-1):\n",
    "        if(arr[a]==0):\n",
    "            arr.pop(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "States = {} \n",
    "root = Node(None, Board,arrCompleted, N_COLORS, H_TUBES,N_TUBES, (-1, -1), 0, 0)\n",
    "graph1 = Graph(root)\n",
    "N_MOVES = graph1.getNeededMoves(root)\n",
    "States = graph1.generateStates(root,H_TUBES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallSortEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = {}\n",
    "        count=0\n",
    "        for i in range(0,N_TUBES): #action space -> generate all possible moves \n",
    "            for j in range(0,N_TUBES):\n",
    "                if (i != j):\n",
    "                    self.action_space[count] = (i,j)\n",
    "                    count +=1\n",
    "        \n",
    "        self.observation_space = States #observation space -> all possible states\n",
    "        self.reset()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        for i in range(len(self.board[0])-1,-1,-1):\n",
    "            for col in self.board:\n",
    "                print(col[i],\" \",end='')\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        info = {}\n",
    "        if(self.checkGameOver()):\n",
    "            done = True\n",
    "            reward = (N_MOVES*0.1) + 1.0\n",
    "            return self.board, reward , done ,info\n",
    "        else: \n",
    "            done = False\n",
    "            reward = -0.1\n",
    "        \n",
    "        if(self.checkValidMove(action)):\n",
    "            self.moveBall(action)   \n",
    "\n",
    "        return self.board, reward , done ,info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = copy.deepcopy(DEFAULT_BOARD)\n",
    "        return self.board\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def checkGameOver(self):\n",
    "        numColsCompleted = 0\n",
    "        for col in self.board:\n",
    "            if(col.count(col[0]) == len(col) and col[0] != 0):\n",
    "                numColsCompleted += 1\n",
    "        if (numColsCompleted == N_COLORS):\n",
    "            return True\n",
    "        else: return False\n",
    "    \n",
    "    def checkValidMove(self,move):\n",
    "        fromCol,toCol = move\n",
    "        fromIndex = -1\n",
    "        toIndex = -1\n",
    "        for number in range(len(self.board[fromCol])-1,-1,-1):\n",
    "            if(self.board[fromCol][number] != 0):\n",
    "                fromIndex = number\n",
    "                break\n",
    "                    \n",
    "        for number in range(0,len(self.board[toCol])):\n",
    "            if(self.board[toCol][number] == 0):\n",
    "                toIndex = number  \n",
    "                break\n",
    "    \n",
    "        if(fromIndex == -1 or toIndex == -1):\n",
    "            return False\n",
    "        \n",
    "        if(toIndex != 0):\n",
    "            if(self.board[fromCol][fromIndex] != self.board[toCol][toIndex-1]):\n",
    "                return False\n",
    "        \n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def moveBall(self,move):\n",
    "        fromCol,toCol = move\n",
    "        fromIndex = -1\n",
    "        toIndex = -1\n",
    "        for number in range(len(self.board[fromCol])-1,-1,-1):\n",
    "            if(self.board[fromCol][number] != 0):\n",
    "                fromIndex = number\n",
    "                break\n",
    "                    \n",
    "        for number in range(0,len(self.board[toCol])):\n",
    "            if(self.board[toCol][number] == 0):\n",
    "                toIndex = number  \n",
    "                break\n",
    "                \n",
    "        ball = self.board[fromCol][fromIndex]\n",
    "        self.board[fromCol][fromIndex] = 0\n",
    "        self.board[toCol][toIndex] = ball\n",
    "        \n",
    "def get_key(val,my_dict):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BallSortEnv()\n",
    "action_space_size = len(env.action_space)\n",
    "state_space_size = len(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learning\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 100 # but it won't go higher than 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "exploration_decay_rate = 0.1 #if we decrease it, will learn slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarsa\n",
    "\n",
    "#Defining the different parameters\n",
    "epsilon = 0.01\n",
    "total_episodes = 5000\n",
    "max_steps = 100\n",
    "alpha = 0.1 #0.85\n",
    "gamma = 0.99 #0.95\n",
    "  \n",
    "#Initializing the Q-matrix\n",
    "Q = np.zeros((state_space_size, action_space_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    state = 0\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration -exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            numAction = np.argmax(q_table[state,:])\n",
    "            action = env.action_space[numAction]\n",
    "        else:\n",
    "            numAction = random.randint(0,len(env.action_space)-1)\n",
    "            action = env.action_space[numAction]    \n",
    "            \n",
    "            \n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_index = get_key(new_state,env.observation_space)\n",
    "        \n",
    "        q_table[state, numAction] = (1 - learning_rate) * q_table[state, numAction] + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_index,:]))\n",
    "        \n",
    "        state = new_index\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per 10 episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 100)\n",
    "count = 100\n",
    "print(\"********** Average  reward per thousand episodes **********\\n\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r / 100)))\n",
    "    count += 100\n",
    "    \n",
    "# Print updated Q-table\n",
    "#print(\"\\n\\n********** Q-table **********\\n\")\n",
    "#for table in range(0,len(q_table)):\n",
    "    #print(table,\" : \",q_table[table])\n",
    "    \n",
    "\n",
    "\n",
    "for episode in range(2):\n",
    "    env.reset()\n",
    "    state = 0\n",
    "    step = 0\n",
    "    action = (-1,-1)\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode,\"\\n\")\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        print(\"State:\",step+1)\n",
    "        print(\"Action:\",action,\"\\n\")\n",
    "        env.render()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        numAction = np.argmax(q_table[state,:])\n",
    "        action = env.action_space[numAction] \n",
    "        \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_index = get_key(new_state,env.observation_space)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Num Moves:\",step)\n",
    "            break\n",
    "        state = new_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "#SARSA\n",
    "\n",
    "\n",
    "def choose_action(state):\n",
    "    numAction=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        numAction = random.randint(0,len(env.action_space)-1)   \n",
    "    else:\n",
    "        numAction = np.argmax(Q[state, :])  \n",
    "    return numAction\n",
    "\n",
    "def update(state, state2, reward, action, action2):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)\n",
    "\n",
    "# SARSA algorithm\n",
    "for episode in range(total_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    state = 0\n",
    "    numAction1 = choose_action(state)   \n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action1 = env.action_space[numAction1] \n",
    "        new_state, reward, done, info = env.step(action1)\n",
    "        \n",
    "        \n",
    "        new_index = get_key(new_state,env.observation_space)\n",
    "        \n",
    "        # Exploration -exploitation trade-off\n",
    "        \n",
    "        numAction2 = choose_action(new_index)\n",
    "        action2 = env.action_space[numAction2] \n",
    "        \n",
    "        update(state,new_index,reward,numAction1,numAction2)\n",
    "            \n",
    "        state = new_index\n",
    "        numAction1 = numAction2\n",
    "        \n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "   \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), total_episodes / 100)\n",
    "count = 100\n",
    "print(\"********** Average  reward per thousand episodes **********\\n\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r / 100)))\n",
    "    count += 100\n",
    "\n",
    "for episode in range(2):\n",
    "    env.reset()\n",
    "    state = 0\n",
    "    step = 0\n",
    "    action = (-1,-1)\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        print(\"State:\",step+1)\n",
    "        print(\"Action:\",action,\"\\n\")\n",
    "        env.render()\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        numAction = np.argmax(q_table[state,:])\n",
    "        action = env.action_space[numAction] \n",
    "        \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_index = get_key(new_state,env.observation_space)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Num Moves:\",step)\n",
    "            break\n",
    "        state = new_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
