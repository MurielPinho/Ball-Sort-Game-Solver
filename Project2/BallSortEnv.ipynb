{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces, error, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TUBES = 3\n",
    "N_COLORS = 2\n",
    "H_TUBES = 3\n",
    "\n",
    "DEFAULT_BOARD = [\n",
    "    [1,1,2],\n",
    "    [2,1,2],\n",
    "    [0,0,0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallSortEnv(gym.Env):\n",
    "    def __init__(self, alpha=0.02):\n",
    "        self.action_space = {0: (0,1),1:(0,2),2:(1,0),3:(1,2),4:(2,0),5:(2,1)}\n",
    "        self.observation_space = {\n",
    "            0 : \n",
    "[[1, 1, 2], [2, 1, 2], [0, 0, 0]] ,\n",
    "\n",
    "1 : \n",
    "[[1, 1, 0], [2, 1, 2], [2, 0, 0]] ,\n",
    "\n",
    "2 : \n",
    "[[1, 1, 2], [2, 1, 0], [2, 0, 0]] ,\n",
    "\n",
    "3 : \n",
    "[[1, 1, 0], [2, 1, 0], [2, 2, 0]] ,\n",
    "\n",
    "4 : \n",
    "[[1, 0, 0], [2, 1, 1], [2, 2, 0]] ,\n",
    "\n",
    "5 : \n",
    "[[1, 1, 1], [2, 0, 0], [2, 2, 0]] ,\n",
    "\n",
    "6 : \n",
    "[[1, 1, 1], [0, 0, 0], [2, 2, 2]] ,\n",
    "\n",
    "7 : \n",
    "[[1, 1, 1], [2, 2, 0], [2, 0, 0]] ,\n",
    "\n",
    "8 : \n",
    "[[1, 1, 0], [1, 0, 0], [2, 2, 2]] ,\n",
    "\n",
    "9 : \n",
    "[[1, 1, 1], [2, 2, 2], [0, 0, 0]] ,\n",
    "\n",
    "10 : \n",
    "[[1, 0, 0], [1, 1, 0], [2, 2, 2]] ,\n",
    "\n",
    "11 : \n",
    "[[1, 1, 0], [2, 2, 2], [1, 0, 0]] ,\n",
    "\n",
    "12 : \n",
    "[[0, 0, 0], [1, 1, 1], [2, 2, 2]] ,\n",
    "\n",
    "13 : \n",
    "[[1, 0, 0], [2, 2, 2], [1, 1, 0]] ,\n",
    "\n",
    "14 : \n",
    "[[2, 0, 0], [1, 1, 1], [2, 2, 0]] ,\n",
    "\n",
    "15 : \n",
    "[[0, 0, 0], [2, 2, 2], [1, 1, 1]] ,\n",
    "\n",
    "16 : \n",
    "[[2, 2, 0], [1, 1, 1], [2, 0, 0]] ,\n",
    "\n",
    "17 : \n",
    "[[2, 0, 0], [2, 2, 0], [1, 1, 1]] ,\n",
    "\n",
    "18 : \n",
    "[[2, 2, 2], [1, 1, 1], [0, 0, 0]] ,\n",
    "\n",
    "19 : \n",
    "[[2, 2, 0], [2, 0, 0], [1, 1, 1]] ,\n",
    "\n",
    "20 : \n",
    "[[2, 2, 2], [1, 1, 0], [1, 0, 0]] ,\n",
    "\n",
    "21 : \n",
    "[[2, 2, 2], [0, 0, 0], [1, 1, 1]] ,\n",
    "\n",
    "22 : \n",
    "[[2, 2, 2], [1, 0, 0], [1, 1, 0]] ,\n",
    "        }\n",
    "        self.alpha = alpha\n",
    "        self.reset()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(self.board)\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        if(self.checkGameOver()):\n",
    "            done = True\n",
    "            reward = 1.4\n",
    "        else: \n",
    "            done = False\n",
    "            reward = -0.1\n",
    "        \n",
    "        if(self.checkValidMove(action)):\n",
    "            self.moveBall(action)\n",
    "        \n",
    "        info = {}\n",
    "\n",
    "        return self.board, reward , done ,info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = copy.deepcopy(DEFAULT_BOARD)\n",
    "        return self.board\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def checkGameOver(self):\n",
    "        numColsCompleted = 0\n",
    "        for col in self.board:\n",
    "            if(col.count(col[0]) == len(col) and col[0] != 0):\n",
    "                numColsCompleted += 1\n",
    "        if (numColsCompleted == N_COLORS):\n",
    "            return True\n",
    "        else: return False\n",
    "    \n",
    "    def checkValidMove(self,move):\n",
    "        fromCol,toCol = move\n",
    "        fromIndex = -1\n",
    "        toIndex = -1\n",
    "        for number in range(len(self.board[fromCol])-1,-1,-1):\n",
    "            if(self.board[fromCol][number] != 0):\n",
    "                fromIndex = number\n",
    "                break\n",
    "                    \n",
    "        for number in range(0,len(self.board[toCol])):\n",
    "            if(self.board[toCol][number] == 0):\n",
    "                toIndex = number  \n",
    "                break\n",
    "    \n",
    "        if(fromIndex == -1 or toIndex == -1):\n",
    "            return False\n",
    "        \n",
    "        if(toIndex != 0):\n",
    "            if(self.board[fromCol][fromIndex] != self.board[toCol][toIndex-1]):\n",
    "                return False\n",
    "        \n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def moveBall(self,move):\n",
    "        fromCol,toCol = move\n",
    "        fromIndex = -1\n",
    "        toIndex = -1\n",
    "        for number in range(len(self.board[fromCol])-1,-1,-1):\n",
    "            if(self.board[fromCol][number] != 0):\n",
    "                fromIndex = number\n",
    "                break\n",
    "                    \n",
    "        for number in range(0,len(self.board[toCol])):\n",
    "            if(self.board[toCol][number] == 0):\n",
    "                toIndex = number  \n",
    "                break\n",
    "                \n",
    "        ball = self.board[fromCol][fromIndex]\n",
    "        self.board[fromCol][fromIndex] = 0\n",
    "        self.board[toCol][toIndex] = ball\n",
    "        \n",
    "def get_key(val,my_dict):\n",
    "    for key, value in my_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = BallSortEnv()\n",
    "action_space_size = len(env.action_space)\n",
    "state_space_size = len(env.observation_space)\n",
    "\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300\n",
    "max_steps_per_episode = 20 # but it won't go higher than 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "exploration_decay_rate = 0.1 #if we decrease it, will learn slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Average  reward per thousand episodes **********\n",
      "\n",
      "100 :  0.6530000000000004\n",
      "200 :  0.9980000000000007\n",
      "300 :  0.9940000000000007\n",
      "\n",
      "\n",
      "********** Q-table **********\n",
      "\n",
      "0  :  [ 0.78354572 28.71820189 -0.12548956 -0.12111174 -0.13294534  2.02453402]\n",
      "1  :  [ 2.65509201  1.54506009  2.03078141 30.17355252 -0.05302669 -0.09780271]\n",
      "2  :  [-0.08491871  0.23107347 -0.08678801 -0.08504885 -0.08275207 -0.10567942]\n",
      "3  :  [-0.0623164   4.85632375 31.64368165 -0.0499001  -0.04792594 -0.05367848]\n",
      "4  :  [-0.04982293 -0.07753107 -0.05540748 -0.05698879 -0.05960069 -0.05960069]\n",
      "5  :  [ 3.57998687e-02 -2.89848471e-02 -6.01766957e-02  3.31286787e+01\n",
      "  2.48686352e+00 -3.43900000e-02]\n",
      "6  :  [ 0.3794      0.37143568  1.29341598  2.15484858 34.62867849  0.        ]\n",
      "7  :  [-0.01       -0.01       -0.01       -0.01893999 -0.019      -0.01      ]\n",
      "8  :  [0. 0. 0. 0. 0. 0.]\n",
      "9  :  [0.14 0.   0.   0.   0.   0.  ]\n",
      "10  :  [0. 0. 0. 0. 0. 0.]\n",
      "11  :  [0. 0. 0. 0. 0. 0.]\n",
      "12  :  [0. 0. 0. 0. 0. 0.]\n",
      "13  :  [0. 0. 0. 0. 0. 0.]\n",
      "14  :  [0. 0. 0. 0. 0. 0.]\n",
      "15  :  [0. 0. 0. 0. 0. 0.]\n",
      "16  :  [0. 0. 0. 0. 0. 0.]\n",
      "17  :  [0. 0. 0. 0. 0. 0.]\n",
      "18  :  [0. 0. 0. 0. 0. 0.]\n",
      "19  :  [0. 0. 0. 0. 0. 0.]\n",
      "20  :  [0. 0. 0. 0. 0. 0.]\n",
      "21  :  [0. 0. 0. 0. 0. 0.]\n",
      "22  :  [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    state = 0\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration -exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            numAction = np.argmax(q_table[state,:])\n",
    "            action = env.action_space[numAction]\n",
    "        else:\n",
    "            numAction = random.randint(0,5)\n",
    "            #while(not env.checkValidMove(env.action_space[numAction])):\n",
    "                #numAction = random.randint(0,5)\n",
    "            action = env.action_space[numAction]    \n",
    "            \n",
    "            \n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_index = get_key(new_state,env.observation_space)\n",
    "        \n",
    "        q_table[state, numAction] = (1 - learning_rate) * q_table[state, numAction] + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_index,:]))\n",
    "        \n",
    "        state = new_index\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True: \n",
    "            break\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per 10 episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 100)\n",
    "count = 100\n",
    "print(\"********** Average  reward per thousand episodes **********\\n\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r / 100)))\n",
    "    count += 100\n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********** Q-table **********\\n\")\n",
    "for table in range(0,len(q_table)):\n",
    "    print(table,\" : \",q_table[table])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
